import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F

import pytorch_lightning as pl

from collections import OrderedDict

from torchvision.datasets import CIFAR10

# 1ï¸âƒ£ CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

batch_size = 128

# ìƒˆë¡œìš´ í‚¤ë¥¼ ì ìš©í•œ state_dict ë¡œë“œ

class LinearClassifier(pl.LightningModule):
    def __init__(self, encoder_location, num_classes=10):
        super().__init__()
        self.encoder = self.load_encoder(encoder_location)
        self.fc = nn.Linear(512, num_classes)  # ResNet18ì˜ feature_dim=512

    def forward(self, x):
        with torch.no_grad():  # Encoder ë¶€ë¶„ì€ gradient ê³„ì‚° ì•ˆ í•¨
            features = self.encoder(x)
            features = torch.flatten(features, start_dim=1)  # Flatten the features
        return self.fc(features)
    
    def load_encoder(self, encoder_location):
        checkpoint = torch.load(encoder_location, map_location='cuda')
        new_state_dict = OrderedDict()
        key_map = {
            "0.": "conv1.",
            "1.": "bn1.",
            "4.": "layer1.",
            "5.": "layer2.",
            "6.": "layer3.",
            "7.": "layer4."
        }

        for k, v in checkpoint.items():
            new_key = k
            for old, new in key_map.items():
                if k.startswith(old):
                    new_key = k.replace(old, new, 1)
                    break
            new_state_dict[new_key] = v
        
        # 1ï¸âƒ£ ResNet-18 ë¶ˆëŸ¬ì˜¤ê¸° (pretrained=False ëª…ì‹œ)
        encoder = torchvision.models.resnet18(pretrained=False)  # ë„¤ê°€ ì‚¬ìš©í•œ encoder êµ¬ì¡°ë¡œ ë³€ê²½í•´ì•¼ í•¨

        # 2ï¸âƒ£ SimCLR Encoder ë¶ˆëŸ¬ì˜¤ê¸°
        encoder.load_state_dict(new_state_dict, strict=False)

        # 3ï¸âƒ£ Encoderì˜ ë§ˆì§€ë§‰ fc ë ˆì´ì–´ ì œê±° (feature extractorë§Œ ì‚¬ìš©)
        encoder = nn.Sequential(*list(encoder.children())[:-1])  # ğŸ”¥ ë§ˆì§€ë§‰ FC ì œê±°!

        # 4ï¸âƒ£ Encoder freeze (í•™ìŠµ X)
        encoder = encoder.cuda()  # GPUë¡œ ì´ë™
        for param in encoder.parameters():
            param.requires_grad = False  # encoderì˜ ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ (freeze)

        return encoder
    
    def training_step(self, batch, batch_idx):
        images, labels = batch
        images, labels = images.cuda(), labels.cuda()

        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)

        preds = torch.argmax(outputs, dim=1)  # ğŸ”¥ ê°€ì¥ í™•ë¥  ë†’ì€ class ì„ íƒ
        acc = (preds == labels).float().mean()  # ğŸ”¥ Accuracy ê³„ì‚°

        self.log("train_loss", loss, prog_bar=True)
        self.log("train_acc", acc, prog_bar=True)  # ğŸ”¥ Accuracy ë¡œê·¸ ì¶”ê°€!

        # Log the loss value
        return loss
    
    def validation_step(self, batch, batch_idx):
        images, labels = batch
        images, labels = images.cuda(), labels.cuda()

        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)

        preds = torch.argmax(outputs, dim=1)  # ğŸ”¥ ê°€ì¥ í™•ë¥  ë†’ì€ class ì„ íƒ
        acc = (preds == labels).float().mean()  # ğŸ”¥ Accuracy ê³„ì‚°

        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", acc, prog_bar=True)  # ğŸ”¥ Accuracy ë¡œê·¸ ì¶”ê°€!

        # Log the loss value
        return loss

    def configure_optimizers(self):
        # 4ï¸âƒ£ Linear Classifier í•™ìŠµì„ ìœ„í•œ optimizer ì„¤ì •
        optimizer = optim.Adam(self.fc.parameters(), lr=0.001)
        return optimizer
    
    def train_dataloader(self):
        train_dataset = CIFAR10(
            root='./../data',
            train=True,
            transform=transform, 
            download=True
        )

        train_loader = torch.utils.data.DataLoader(
            dataset=train_dataset,
            batch_size=batch_size,
            num_workers=4,
            persistent_workers=True,
            shuffle=True)
        return train_loader
    
    def val_dataloader(self):
        val_dataset = CIFAR10(
            root='./../data',
            train=False,
            transform=transform
        )

        val_loader = torch.utils.data.DataLoader(
            dataset=val_dataset,
            batch_size=batch_size,
            num_workers=4,
            persistent_workers=True,
            shuffle=False)
        return val_loader

if __name__ == "__main__":
    torch.set_float32_matmul_precision('medium')
    # 1ï¸âƒ£ ëª¨ë¸ ì´ˆê¸°í™”
    model = LinearClassifier(encoder_location="encoder_5_500.pth")
    # 2ï¸âƒ£ Trainer ì„¤ì •
    trainer = pl.Trainer(max_epochs=50, accelerator="gpu", devices=1)
    # 3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ
    trainer.fit(model)
    # 4ï¸âƒ£ ëª¨ë¸ ì €ì¥
    trainer.save_checkpoint("linear_classifier_5_500.ckpt")  # âœ… Lightning ê¶Œì¥ ë°©ì‹
