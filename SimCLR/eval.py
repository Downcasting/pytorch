import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F

import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger

import datetime

from collections import OrderedDict

from torchvision.datasets import CIFAR10

# 1ï¸âƒ£ CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

batch_size = 128

# ìƒˆë¡œìš´ í‚¤ë¥¼ ì ìš©í•œ state_dict ë¡œë“œ

class LinearClassifier(pl.LightningModule):
    def __init__(self, encoder_location, num_classes=10, resnet18=True):
        super().__init__()
        self.resnet18 = resnet18
        self.encoder = self.load_encoder(encoder_location)
        feature_dim = 512 if resnet18 else 2048  # ResNet18 feature_dim=512, ResNet50 feature_dim=2048
        self.fc = nn.Linear(feature_dim, num_classes)

    def forward(self, x):
        with torch.no_grad():  # Encoder ë¶€ë¶„ì€ gradient ê³„ì‚° ì•ˆ í•¨
            features = self.encoder(x)
            features = torch.flatten(features, start_dim=1)  # Flatten the features
        return self.fc(features)
    
    def load_encoder(self, encoder_location):
        checkpoint = torch.load(encoder_location, map_location='cuda')
        new_state_dict = OrderedDict()
        key_map = {
            "0.": "conv1.",
            "1.": "bn1.",
            "4.": "layer1.",
            "5.": "layer2.",
            "6.": "layer3.",
            "7.": "layer4."
        }

        for k, v in checkpoint.items():
            new_key = k
            for old, new in key_map.items():
                if k.startswith(old):
                    new_key = k.replace(old, new, 1)
                    break
            new_state_dict[new_key] = v
        
        # 1ï¸âƒ£ ResNet-18 / ResNet-50 ë¶ˆëŸ¬ì˜¤ê¸° (pretrained=False ëª…ì‹œ)
        if self.resnet18:
            encoder = torchvision.models.resnet18(weights=None)  # ResNet-18 ì‚¬ìš©
        else:
            encoder = torchvision.models.resnet50(weights=None)  # ResNet-50 ì‚¬ìš©

        # 2ï¸âƒ£ SimCLR Encoder ë¶ˆëŸ¬ì˜¤ê¸°
        encoder.load_state_dict(new_state_dict, strict=False)

        # 3ï¸âƒ£ Encoderì˜ ë§ˆì§€ë§‰ fc ë ˆì´ì–´ ì œê±° (feature extractorë§Œ ì‚¬ìš©)
        encoder = nn.Sequential(*list(encoder.children())[:-1])  # ğŸ”¥ ë§ˆì§€ë§‰ FC ì œê±°!

        # 4ï¸âƒ£ Encoder freeze (í•™ìŠµ X)
        encoder = encoder.cuda()  # GPUë¡œ ì´ë™
        for param in encoder.parameters():
            param.requires_grad = False  # encoderì˜ ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ (freeze)

        return encoder
    
    def training_step(self, batch, batch_idx):
        images, labels = batch
        images, labels = images.cuda(), labels.cuda()

        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)

        preds = torch.argmax(outputs, dim=1)  # ğŸ”¥ ê°€ì¥ í™•ë¥  ë†’ì€ class ì„ íƒ
        acc = (preds == labels).float().mean()  # ğŸ”¥ Accuracy ê³„ì‚°

        self.log("train_loss", loss, prog_bar=True, logger=True)
        self.log("train_acc", acc, prog_bar=True, logger=True)  # ğŸ”¥ Accuracy ë¡œê·¸ ì¶”ê°€!

        # Log the loss value
        return loss
    
    def validation_step(self, batch, batch_idx):
        images, labels = batch
        images, labels = images.cuda(), labels.cuda()

        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)

        preds = torch.argmax(outputs, dim=1)  # ğŸ”¥ ê°€ì¥ í™•ë¥  ë†’ì€ class ì„ íƒ
        acc = (preds == labels).float().mean()  # ğŸ”¥ Accuracy ê³„ì‚°

        self.log("val_loss", loss, prog_bar=True, logger=True)
        self.log("val_acc", acc, prog_bar=True, logger=True)  # ğŸ”¥ Accuracy ë¡œê·¸ ì¶”ê°€!

        # Log the loss value
        return loss

    def configure_optimizers(self):
        # 4ï¸âƒ£ Linear Classifier í•™ìŠµì„ ìœ„í•œ optimizer ì„¤ì •
        optimizer = optim.Adam(self.fc.parameters(), lr=0.001)
        scheduler = {
            "scheduler" : optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True),
            "monitor" : "val_loss",
            "interval" : "epoch",
            "frequency" : 1
        }
        return {"optimizer": optimizer, "lr_scheduler": scheduler}
    
    def train_dataloader(self):
        train_dataset = CIFAR10(
            root='./../data',
            train=True,
            transform=transform, 
            download=True
        )

        train_loader = torch.utils.data.DataLoader(
            dataset=train_dataset,
            batch_size=batch_size,
            num_workers=4,
            persistent_workers=True,
            shuffle=True)
        return train_loader
    
    def val_dataloader(self):
        val_dataset = CIFAR10(
            root='./../data',
            train=False,
            transform=transform
        )

        val_loader = torch.utils.data.DataLoader(
            dataset=val_dataset,
            batch_size=batch_size,
            num_workers=4,
            persistent_workers=True,
            shuffle=False)
        return val_loader
    
    def training_epoch_end(self, outputs):
        current_lr = self.trainer.optimizers[0].param_groups[0]["lr"]
        self.log("current_lr", current_lr, prog_bar=True, logger=True)
    
    def on_fit_end(self):
        with open(f"eval info.txt", "a") as f:
            f.write(f"----------------------------------------\n")
            f.write(f"[Version: {version}]\n\n")
            f.write(f"Date: {datetime.datetime.now()}\n")
            f.write(f"Batch Size: {batch_size}\n")
            f.write(f"Max Epochs: {max_epochs}\n")
            f.write(f"Total Accuracy: {self.trainer.callback_metrics['val_acc']*100:.2f}%\n")
            f.write(f"----------------------------------------\n\n")

if __name__ == "__main__":

    ### ResNet-18 or ResNet-50 ###
    usingResNet18 = True # ResNet-18 ì‚¬ìš© ì—¬ë¶€
    version = 7 # ë²„ì „
    max_epochs = 30 # ìµœëŒ€ ì—í­
    ##############################

    logger = TensorBoardLogger("tb_logs", name="SimCLR Eval", version=f"v{version}")

    torch.set_float32_matmul_precision('medium')
    # 1ï¸âƒ£ ëª¨ë¸ ì´ˆê¸°í™”
    model = LinearClassifier(encoder_location=f"v{version}_encoder.pth", resnet18=usingResNet18)
    # 2ï¸âƒ£ Trainer ì„¤ì •
    trainer = pl.Trainer(max_epochs=max_epochs, accelerator="gpu", devices=1, logger=logger)
    # 3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ
    trainer.fit(model)
    # 4ï¸âƒ£ ëª¨ë¸ ì €ì¥
    # trainer.save_checkpoint("linear_classifier_300.ckpt")  # âœ… Lightning ê¶Œì¥ ë°©ì‹
